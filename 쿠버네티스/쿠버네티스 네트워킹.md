## 1. 파드 네트워킹
### 1.1 (네트워킹 관점에서)파드는
- ![image](https://github.com/user-attachments/assets/2d568bb3-5fed-4552-9cf9-eac40cffedc1)
- 한 개 이상의 컨테이너를 구성하고 같은 host와 network 스택을 공유
- 네트워크 스택을 공유한다는것은?
  - 같은 네트워크 인터페이스를 사용
  - localhost:포트를 통해 파드내 컨테이너간 통신 가능
```
이해하고 가자
veth(virtual ethernet interface):
veth는 쌍으로 만들어지며 네트워크 네임스페이스들을 터널로서 연결하거나,
물리 디바이스와 다른 네트워크 네임스페이스의 장비를 연결하는 용도로 사용할 수 있다.

eth0 :
eth0는 리눅스 기반 운영체제에서 첫 번째 이더넷 네트워크 인터페이스를 의미하는 이름
이는 시스템이 처음으로 감지한 네트워크 인터페이스 카드(NIC)에 할당되는 표준 명칭
모든 컨테이너의 Network Interface는 컨테이너 런타임이 제공하는 네트워크 브리지(bridge)와 연결되어 있고
컨테이너에선 이를 위해 veth pair로 생성된 네트워크 인터페이스 중 하나가 컨테이너의 eth다
(pair의 반대쪽은 브릿지)

그렇다면 파드내 컨테이너로의 통신, 컨테이너간 통신을 위해서는
pod
 |
bridge
 |
veth
| | |
e e e
t t t
h h h
```
- ![image](https://github.com/user-attachments/assets/1f6a4842-b07f-41f6-ba5a-ff225fed8982)
```
참고로
“pause” 컨테이너는 파드의 생애 주기 동안 동일한 네트워크 네임스페이스(Network Namespace)를 유지시켜 주는 역할을 한다.
만약, 파드가 완전히 제거되는것이 아닌 내부의 컨테이너가 어떤 이유로 재시작 되어도
파드가 동일한 네트워크 네임스페이스(동일한 IP)를 유지할 수 있는 이유다.
pause 컨테이너의 Lifecycle은 존재 목적에서 유추할 수 있듯이 파드와 동일한 Lifecycle을 가지며, 만약 파드가 종료되면 같이 종료된다.
```
### 1.2 서로 다른 호스트의 파드와 통신
- ![image](https://github.com/user-attachments/assets/1f13a4bb-6e12-42a6-b997-4a503adf315c)
- 클러스터에 존재하는 파드는 모두 고유한 네트워크 인터페이스와 IP가 존재하여 서로 직접 통신할 수 있다.
```
그런데 앞서 얘기한대로라면 n개 호스트내 컨테이너는 위처럼 구성될것이다.
즉 서로 다른 호스트의 브릿지가 같은 주소를 사용한다.
```
- 쿠버네티스는 이 문제를 어떻게 해결하는가?
```
1. 쿠버네티스는 각 노드 bridge의 ip가 겹치지 않는 대역을 할당한다.
2. 그 다음 gateway에 어떤 패킷이 어떤 bridge로 가야하는지 라우팅 테이블 정의
```
- ![image](https://github.com/user-attachments/assets/fa1a7267-43fe-4196-9137-4bbdf74405a0)

### 1.3 nat없는 flat한 통신
- 위 구조대로라면 클러스터에 존재하는 파드는 모두 고유한 네트워크 인터페이스와 IP가 존재하여 서로 직접 통신할 수 있다.
- 호스트내 모든 파드는 호스트의 브릿지와 연결되어 있다.
  - 이를 통해 호스트내 서로 다른 컨테이너 통신간 nat가 필요하지 않다.
- 그렇다면 서로 다른 노드의 파드간 통신에서도 nat가 없어야 효율적이다.
  - overlay network
- ![image](https://github.com/user-attachments/assets/6965b51c-201f-4623-b27b-27abf1994a99)
```
컨테이너에서 출발한 패킷은 호스트의 물리 네트워크 인터페이스에 도착하여 오버레이 네트워크에 의해 “캡슐화(Encapsulation)” 되고 도착지에서 “디캡슐화(Decapsulation)” 되면서 패킷이 흐를 수 있게 된다.
k8s에서는”Weave Net, Flannel, Calico” 와 같은 “Container Network Interface(CNI)”를 통해서 이를 구현한다.
``` 


### 1.4 정리
```
각 호스트에서
하나의 파드는 서로 다른 컨테이너 존재
- 같은 네트워크 인터페이스 사용으로 localhost 통신

동일 호스트내 서로 다른 파드는 호스트의 브릿지와 veth pair로 연결
- 동일 브릿지를 통해 서로 통신 가능
- 호스트내 서로 다른 파드는 모두 호스트의 브릿지와 veth pair로 연결되어있고 같은 서브넷 대역이니 nat없이 통신

서로 다른 호스트의 브릿지는 서로 다른 대역을 갖는다
그리고 cni를 통한 overlay network를 통해 서로 다른 호스트내 파드간 통신에서 nat 없이 통신
```

### 2. 서비스(클러스터 내/외부)
- ![image](https://github.com/user-attachments/assets/1d479ef4-3121-43a2-b161-1183e87731cd)

### 2-1. 먼저 클러스터 내부 통신을 살펴보자
- 파드는 언제든지 재생성되고 ip가 유동적
  - 따라서 쿠버네티스는 Service를 통해 클러스터에 배포된 각 파드의 Endpoint를 관리
  - 클러스터 내부에서 파드로의 진입점을 제공하는것이 clusterIP타입 서비스
- kube-proxy는 iptables라는 커널의 User space의 프로그램을 통해 각 서비스IP와 서비스에 연결된 엔드포인트를 관리
  - iptables에 service IP -> Endpoint 01 과 같은 규칙을 구성해둔다.
    - 즉 특정 service의 ip는 어떤 endpoint로 향하라.
  - service ip와 엔드포인트를 매핑하여 destination nat 수행
  - 이후, netfilter를 통해 네트워크 패킷 로드밸런싱
- netfilter는 패킷이 호스트의 물리 네트워크에 도착하면 패킷의 destination ip 주소 & 포트가 iptables가 관리하는 규칙(DNAT 규칙) 중 해당하는 규칙이 존재하는지 확인
  - 매칭되는 규칙이 존재하면 n개 규칙 중 랜덤하게 엔드포인트로 전달
    - 하나의 service에는 여러 파드가 연결되어있을것이다.
    - serviceIP 01 -> endpoint 01
    - serviceIP 02 -> endpoint 02
    - serviceIP 03 -> endpoint 03
    - 이것이 iptables의 한계이기도 하다.
  - 매칭되는 규칙을 찾은 뒤 전달할때 DNAT를 수행하니
    - 이제 기존 도착지 주소를 endpoint에서 확인할 수 있는 실제 최종 도착지 주소(파드 ip 및 포트)로 변경하여 패킷 전달
### 2-2. 외부로부터 클러스터로 들어오는 요청
- ![image](https://github.com/user-attachments/assets/5816089f-06e7-43f0-bfcf-ee8d4d709995)
- 로드밸런서 타입 service는 nodePort 타입 서비스를 한 번 더 래핑한것이다
  - 즉 타겟그룹이 nodePort 서비스가 된다는 것
- 그렇다면 로드밸런서를 통해 들어온 패킷은 뒷단 타겟그룹으로 전달될텐데 이는 결국 외부에서 nodePort로 접근하는거랑 파드에 접근하는 과정은 같을것
  - 특정 노드 물리 인터페이스에 할당된 ip에 대해 특정 포트로 들어오는 패킷은 kube-proxy에 의해 cluster IP로 변환
  - nodePort도 결국 서비스의 타입 중 하나고 그렇기 때문에 서비스는 clusterIP를 갖기때문에 가능
  - 이제 다시 iptables를 거쳐 DNAT되어 내부 통신 과정을 거친다.
- 즉 LoadBalancer > NodePort > ClutserIP > Pod IP(Endpoint)

### 2-3. ingress controller를 사용한 외부 트래픽 흐름
- ![image](https://github.com/user-attachments/assets/511b1a9f-2695-4adf-a8e7-1fdc9c11768f)
- loadbalancer 타입 서비스도 결국 L4 layer에서 ip/port 기반 부하분산이다.
- L7 url path 기반 부하분산과 TLS Termination은 수행할 수 없다.
- 따라서 ingress를 필요로 할 수 있다.
- 트래픽을 실제로 처리하는것은 Ingress controller pod다
  - 즉 lb -> nodeport -> clusterip -> pod(ingress controller pod)까지 먼저 트래픽이 들어오는것이다.
- ingress controller pod로 트래픽이 도착을 하면 nginx.conf에 설정된 upstream 설정을 통해 적절한 백앤드 서비스로 패킷을 전달한다.


